# ğŸ“š Clean RAG System

A production-ready Retrieval Augmented Generation (RAG) system with smart table handling, strict citation requirements, and comprehensive evaluation.

## ğŸŒŸ Features

- **Smart PDF Ingestion**: Detects and separately processes tables vs text
- **Table Summarization**: Generates LLM-friendly summaries of tables
- **Vector Storage**: ChromaDB (local) with easy swap to Pinecone
- **Strict Citations**: System prompt enforces context-only answers
- **No Hallucination**: Built-in guardrails against making up information
- **RAGAS Evaluation**: Measure faithfulness and other quality metrics
- **Streamlit UI**: Simple chat interface for document Q&A

## ğŸ“ Project Structure

```
clean-rag/
â”œâ”€â”€ .env                    # API keys (create from .env.template)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ app.py                  # Streamlit chat interface
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py         # Package initialization
â”‚   â”œâ”€â”€ ingestion.py        # PDF loading, table detection, chunking
â”‚   â”œâ”€â”€ retrieval.py        # Vector store setup and retrieval
â”‚   â”œâ”€â”€ chain.py            # RAG chain with citation prompts
â”‚   â””â”€â”€ evaluation.py       # RAGAS evaluation scripts
â””â”€â”€ chroma_db/              # Local vector database (auto-created)
```

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
# Create project directory
mkdir clean-rag && cd clean-rag

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
# Copy template
cp .env.template .env

# Edit .env and add your OpenAI API key
# OPENAI_API_KEY=sk-your-key-here
```

### 3. Run the Application

```bash
# Start the Streamlit UI
streamlit run app.py
```

Then open http://localhost:8501 in your browser.

## ğŸ’» Usage Examples

### Programmatic Usage

```python
from src.ingestion import ingest_pdf
from src.chain import create_rag_chain

# 1. Ingest a PDF
chunks = ingest_pdf("my_document.pdf")
print(f"Created {len(chunks)} chunks")

# 2. Create RAG chain and add documents
rag = create_rag_chain()
rag.add_documents(chunks)

# 3. Ask questions
response = rag.invoke("What are the key findings?")
print(response.answer)

# 4. View sources
for doc in response.sources:
    print(f"- {doc.metadata['source']}: {doc.page_content[:100]}...")
```

### Command Line Ingestion

```bash
# Ingest a PDF from command line
python -m src.ingestion path/to/document.pdf
```

### Run Evaluation

```bash
# Full evaluation with all RAGAS metrics
python -m src.evaluation --full

# Quick faithfulness-only test
python -m src.evaluation --quick
```

## âš™ï¸ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | Required | Your OpenAI API key |
| `LLM_MODEL` | `gpt-4o-mini` | Model for generation |
| `EMBEDDING_MODEL` | `text-embedding-3-small` | Model for embeddings |
| `CHUNK_SIZE` | `1000` | Text chunk size |
| `CHUNK_OVERLAP` | `200` | Overlap between chunks |
| `CHROMA_PERSIST_DIR` | `./chroma_db` | ChromaDB storage path |
| `CHROMA_COLLECTION_NAME` | `clean_rag_docs` | Collection name |

### Switching to Pinecone

1. Install Pinecone: `pip install pinecone-client langchain-pinecone`

2. Add to `.env`:
```bash
PINECONE_API_KEY=your-key
PINECONE_INDEX_NAME=clean-rag-index
```

3. Change provider in code:
```python
from src.retrieval import get_vector_store_provider

# Use Pinecone instead of ChromaDB
provider = get_vector_store_provider("pinecone")
```

## ğŸ“Š Evaluation Metrics

The system uses RAGAS for evaluation:

| Metric | Description | Target |
|--------|-------------|--------|
| **Faithfulness** | Answer only uses context info | > 0.9 |
| **Answer Relevancy** | Answer addresses the question | > 0.8 |
| **Context Precision** | Retrieved docs are relevant | > 0.8 |
| **Context Recall** | All needed info was retrieved | > 0.8 |

## ğŸ”§ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF Upload    â”‚â”€â”€â”€â”€â–¶â”‚   Ingestion     â”‚â”€â”€â”€â”€â–¶â”‚   ChromaDB      â”‚
â”‚                 â”‚     â”‚  (unstructured) â”‚     â”‚  (embeddings)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Answer       â”‚â—€â”€â”€â”€â”€â”‚   RAG Chain     â”‚â—€â”€â”€â”€â”€â”‚   Retriever     â”‚
â”‚  (with cites)   â”‚     â”‚  (LangChain)    â”‚     â”‚   (top-k)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    RAGAS        â”‚
â”‚   Evaluation    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›¡ï¸ Anti-Hallucination Design

The system uses multiple layers to prevent hallucination:

1. **System Prompt**: Explicitly commands "Answer only based on provided context"
2. **Temperature 0**: Deterministic outputs reduce creativity/hallucination
3. **Source Citations**: Forces attribution to specific sources
4. **"I don't know"**: Trained to admit when context lacks answer
5. **RAGAS Evaluation**: Measures faithfulness to detect issues

## ğŸ“ License

MIT License - feel free to use and modify.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run evaluation to ensure quality
5. Submit a pull request
